---
title: "Regression Methods Project"
format: html
header-includes: 
  - \usepackage{xcolor}
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{bm}
df_agg-engine: xelatex
---
```{r, message=FALSE, warning=FALSE}
#| echo: false
# Load necessary libraries
library(tidyverse)
library(glue)
library(viridis)
library(ggplot2)
library(dplyr)
library(performance)     # For check_model, etc.
library(lme4)            # For GLMM
library(broom)
library(broom.mixed)     # Tidy for mixed models
library(car)             # For VIF
library(lattice)         # For random-effects dotplot
library(influence.ME)

```

To reproduce this code it is necessary to have the files data-agg.csv and df-time.csv. Please set the path to where 'data-agg.csv' and 'df-time.csv' are located.

#### **1. Introduction and Exploratory Data Analysis**

```{r, message=FALSE, warning=FALSE}
#| echo: false
# Read the aggregated dataset
df_agg <- read_csv("/Users/graceaverell/Desktop/EPFL/regression methods/coin data/analyses/data-agg.csv") %>%
  mutate(
    person = factor(person),
    coin = factor(coin)
  )

#df_agg <- read_csv("C:/Users/mimav/OneDrive/Desktop/regression/data-agg.csv") %>%
#  mutate(
#    person = factor(person),
#    coin = factor(coin)
#  )

# Read the time-resolved dataset
df_agg_time <- read_csv("/Users/graceaverell/Desktop/EPFL/regression methods/coin data/analyses/df-time.csv") %>%
  mutate(
    person = factor(person),
    coin = factor(coin),
    toss_start = factor(toss_start),
    toss_end = factor(toss_end)
  ) %>%
  arrange(person, sequence_id, toss_number) %>%
  group_by(person, sequence_id) %>%
  mutate(
    lag_1 = lag(toss_end, 1),  # Outcome of the previous flip
    lag_2 = lag(toss_end, 2),  # Outcome of two flips ago
    lag_3 = lag(toss_end, 3)   # Outcome of three flips ago
  ) %>%
  ungroup() %>%
  filter(!is.na(lag_1) & !is.na(lag_2) & !is.na(lag_3))  # Remove NA rows for lags

#df_agg_time <- read_csv("C:/Users/mimav/OneDrive/Desktop/regression/df_agg-time.csv") %>%
#  mutate(
#    person     = factor(person),
#    coin       = factor(coin),
#    toss_start = factor(toss_start),
#    toss_end   = factor(toss_end)
#  ) %>%
#  arrange(person, sequence_id, toss_number) %>%
#  group_by(person, sequence_id) %>%
#  mutate(
#    lag_1 = lag(toss_end, 1),
#    lag_2 = lag(toss_end, 2),
#    lag_3 = lag(toss_end, 3)
#  ) %>%
#  ungroup()

# Check resulting data frame
head(df_agg_time)

```

##### **1.1. EDA**

**Overall Probability of Landing on the Same Side**

```{r, message=FALSE, warning=FALSE}
#| echo: false
# Calculate flips where the coin landed on the opposite side
df_agg <- df_agg %>%
  mutate(
    heads_tails = N_start_heads_up - heads_heads,
    tails_tails = N_start_tails_up - tails_heads
  )

# Calculate total flips and probabilities
df_agg <- df_agg %>%
  mutate(
    total_flips = N_start_heads_up + N_start_tails_up,
    total_same_side = heads_heads + tails_tails,
    prob_same_side = total_same_side / total_flips,
    prob_heads_to_heads = heads_heads / N_start_heads_up,
    prob_tails_to_tails = tails_tails / N_start_tails_up
  )

# Summary statistics for probability of landing on the same side}
summary_stats <- df_agg %>%
  summarise(
    mean_prob_same_side = mean(prob_same_side, na.rm = TRUE),
    median_prob_same_side = median(prob_same_side, na.rm = TRUE),
    sd_prob_same_side = sd(prob_same_side, na.rm = TRUE)
  )

print(summary_stats)
```

```{r, message=FALSE, warning=FALSE}
#| echo: false
# t test against p=0.5 
t.test(df_agg$prob_same_side, mu = 0.5, alternative = "two.sided")
```

```{r, message=FALSE, warning=FALSE}
#| echo: false
# Histogram of probabilities of landing on the same side
fig1 <- ggplot(df_agg, aes(x = prob_same_side)) +
  geom_histogram(binwidth = 0.001, color = "black", fill = viridis(5)[2]) +
  labs(
    title = "Histogram of Probability of Landing on the Same Side",
    x = "Probability",
    y = "Frequency"
  ) +
  theme_minimal()

ggsave("fig1.png", plot = fig1, width = 8, height = 6, dpi = 300)
```

**Participant-Level Analysis**

```{r, message=FALSE, warning=FALSE}
#| echo: false
# Calculate participant-level probabilities
participant_probs <- df_agg %>%
  group_by(person) %>%
  summarise(
    total_heads_heads = sum(heads_heads),
    total_tails_tails = sum(tails_tails),
    total_heads_up = sum(N_start_heads_up),
    total_tails_up = sum(N_start_tails_up),
    total_same_side = total_heads_heads + total_tails_tails,
    total_flips = total_heads_up + total_tails_up,
    prob_same_side = total_same_side / total_flips
  )

# Summary statistics at participant level
participant_summary <- participant_probs %>%
  summarise(
    mean_prob = mean(prob_same_side),
    median_prob = median(prob_same_side),
    sd_prob = sd(prob_same_side)
  )

print(participant_summary)

```

```{r, message=FALSE, warning=FALSE}
#| echo: false
# Histogram of probabilities of landing on the same side by participant
fig2 <- ggplot(participant_probs, aes(x = prob_same_side)) +
  geom_histogram(binwidth = 0.001, color = "black", fill = viridis(5)[3]) +
  labs(
    title = "Probability of Landing on the Same Side by Participant",
    x = "Probability",
    y = "Frequency"
  ) +
  theme_minimal()

ggsave("fig2.png", plot = fig2, width = 8, height = 6, dpi = 300)
```

```{r, message=FALSE, warning=FALSE}
#| echo: false
# Identify participants with extreme probabilities
participant_outliers <- participant_probs %>%
  filter(prob_same_side > mean(prob_same_side) + 2 * sd(prob_same_side) |
           prob_same_side < mean(prob_same_side) - 2 * sd(prob_same_side))

print(participant_outliers)

# Calculate probabilities for person/coin combinations
person_coin_probs <- df_agg %>%
  group_by(person, coin) %>%
  summarise(prob_same_side = mean(prob_same_side, na.rm = TRUE), .groups = "drop")

# Identify person/coin combination outliers
person_coin_outliers <- person_coin_probs %>%
  filter(prob_same_side > mean(prob_same_side) + 2 * sd(prob_same_side) |
           prob_same_side < mean(prob_same_side) - 2 * sd(prob_same_side))

print(person_coin_outliers)

```
**Effect of Starting Side**

```{r, message=FALSE, warning=FALSE}
#| echo: false
# Compare probabilities based on starting side
starting_side_probs <- df_agg %>%
  summarise(
    mean_prob_heads_to_heads = mean(prob_heads_to_heads, na.rm = TRUE),
    mean_prob_tails_to_tails = mean(prob_tails_to_tails, na.rm = TRUE),
    sd_prob_heads_to_heads = sd(prob_heads_to_heads, na.rm = TRUE),
    sd_prob_tails_to_tails = sd(prob_tails_to_tails, na.rm = TRUE)
  )

print(starting_side_probs)

```

```{r,message=FALSE, warning=FALSE}
#| echo: false
# Prepare data for plotting
df_agg_long <- df_agg %>%
  select(prob_heads_to_heads, prob_tails_to_tails) %>%
  pivot_longer(
    cols = c(prob_heads_to_heads, prob_tails_to_tails),
    names_to = "starting_side",
    values_to = "probability"
  ) %>%
  mutate(
    starting_side = case_when(
      starting_side == "prob_heads_to_heads" ~ "Heads Up",
      starting_side == "prob_tails_to_tails" ~ "Tails Up"
    )
  )

# Summarize the data by starting side
df_agg_long %>%
  group_by(starting_side) %>%
  summarise(
    mean_prob = mean(probability, na.rm = TRUE),
    sd_prob = sd(probability, na.rm = TRUE)
  ) %>%
  print()

# Perform t-test
t.test(probability ~ starting_side, data = df_agg_long)
```

```{r,message=FALSE,warning=FALSE}
#| echo: false
# Identify outliers by starting side
starting_side_outliers <- df_agg_long %>%
  group_by(starting_side) %>%
  filter(probability > mean(probability) + 2 * sd(probability) |
           probability < mean(probability) - 2 * sd(probability))

print(starting_side_outliers)
```

#### **2. Analysis**

##### 2.1 Simple Logistic Regression Model

```{r, message=FALSE, warning=FALSE}
#| echo: false
# Fit the simplest possible logistic model
model_1 <- glm(
  cbind(heads_heads, N_start_heads_up - heads_heads) ~ 1,
  family = binomial,
  data = df_agg
)

# Print summary
summary(model_1)

# Model diagnostics 
check_model(model_1)


#Refit without outlier participants
df_filtered_2 <- df_agg %>%
  filter(!(person %in% c("TianqiPeng", "JanYang")))

model_1_filtered <- glm(
  cbind(heads_heads, N_start_heads_up - heads_heads) ~ 1,
  family = binomial,
  data = df_filtered_2
)

summary(model_1_filtered)


```

##### 2.2 Random Intercepts for Participants Only

```{r, message=FALSE, warning=FALSE}
#| echo: true

# Mixed-effects logistic regression model with random intercepts for participants
model_2 <- glmer(cbind(total_same_side, total_flips - total_same_side) ~ 1 + 
                             (1 | person),
                           family = binomial, data = df_agg)

# Print summary
summary(model_2)

# Model diagnostics
check_model(model_2)
```

```{r, message=FALSE, warning=FALSE}
#| echo: true
# Which participants, if removed entirely, change the model fit the most?
inf_mod2 <- influence(model_2, group = "person")
summary(inf_mod2)
plot(inf_mod2, which = "cook")

# Extract Cook's distances for each participant
cook_vals <- cooks.distance(inf_mod2)
cook_df <- data.frame(
  person = levels(df_agg$person),
  cooks  = as.numeric(cook_vals)
)

# Sort them descending
cook_df_sorted <- cook_df[order(-cook_df$cooks), ]
head(cook_df_sorted, 10)

# Refit model without the most influential participant and the two most influential participants
df_filtered <- df_agg %>%
  filter(!(person %in% c("TianqiPeng")))

df_filtered_2 <- df_agg %>%
  filter(!(person %in% c("TianqiPeng", "JanYang")))

model_2_filtered <- glmer(
  cbind(total_same_side, total_flips - total_same_side) ~ 1 + 
  (1 | person),
  family = binomial, 
  data = df_filtered
)

model_2_filtered_2 <- glmer(
  cbind(total_same_side, total_flips - total_same_side) ~ 1 + 
  (1 | person),
  family = binomial, 
  data = df_filtered_2
)

summary(model_2_filtered)
summary(model_2_filtered_2)

# Filtering out the two influential participants (TianqiPeng and JanYang) led to lower AIC/BIC and higher log-likelihood, indicating a better-fitting model.

# The random effect variance dropped significantly, confirming that the excluded participants introduced greater variability in the data, which has now been controlled.
# The fixed effect estimates became more precise (lower standard errors) without compromising statistical significance.

# The lower AIC values in the filtered models are partially influenced by the smaller n, however, the consistent improvement across AIC, BIC, log-likelihood, and precision metrics suggests that the filtered models are better specified rather than just benefiting from fewer data points.

## Investigating probability of same side outcome 
# Extract fixed effect estimates (log-odds) for all models
beta_0 <- fixef(model_2)["(Intercept)"]
beta_0_filtered <- fixef(model_2_filtered)["(Intercept)"]
beta_0_filtered_2 <- fixef(model_2_filtered_2)["(Intercept)"]

# Calculate standard errors
se <- sqrt(vcov(model_2)["(Intercept)", "(Intercept)"])
se_filtered <- sqrt(vcov(model_2_filtered)["(Intercept)", "(Intercept)"])
se_filtered_2 <- sqrt(vcov(model_2_filtered_2)["(Intercept)", "(Intercept)"])

# Compute probabilities
prob <- exp(beta_0) / (1 + exp(beta_0))
prob_filtered <- exp(beta_0_filtered) / (1 + exp(beta_0_filtered))
prob_filtered_2 <- exp(beta_0_filtered_2) / (1 + exp(beta_0_filtered_2))

# Display probabilities
cat("Model 2 Probability: ", prob, "\n")
cat("Model 2 Filtered Probability: ", prob_filtered, "\n")
cat("Model 2 Filtered 2 Probability: ", prob_filtered_2, "\n")

# Test differences in log-odds (z-tests)
# Model 2 vs Model 2 Filtered
z_1 <- (beta_0 - beta_0_filtered) / sqrt(se^2 + se_filtered^2)
p_1 <- 2 * (1 - pnorm(abs(z_1)))

# Model 2 vs Model 2 Filtered 2
z_2 <- (beta_0 - beta_0_filtered_2) / sqrt(se^2 + se_filtered_2^2)
p_2 <- 2 * (1 - pnorm(abs(z_2)))

# Model 2 Filtered vs Model 2 Filtered 2
z_3 <- (beta_0_filtered - beta_0_filtered_2) / sqrt(se_filtered^2 + se_filtered_2^2)
p_3 <- 2 * (1 - pnorm(abs(z_3)))

# Display z-scores and p-values for significance testing
cat("Model 2 vs Model 2 Filtered: z =", z_1, ", p =", p_1, "\n")
cat("Model 2 vs Model 2 Filtered 2: z =", z_2, ", p =", p_2, "\n")
cat("Model 2 Filtered vs Model 2 Filtered 2: z =", z_3, ", p =", p_3, "\n")

# Confidence intervals for probabilities
ci <- beta_0 + c(-1.96, 1.96) * se
ci_filtered <- beta_0_filtered + c(-1.96, 1.96) * se_filtered
ci_filtered_2 <- beta_0_filtered_2 + c(-1.96, 1.96) * se_filtered_2

# Transform to probability scale
ci_prob <- exp(ci) / (1 + exp(ci))
ci_prob_filtered <- exp(ci_filtered) / (1 + exp(ci_filtered))
ci_prob_filtered_2 <- exp(ci_filtered_2) / (1 + exp(ci_filtered_2))

# Display confidence intervals
cat("Model 2 95% CI: [", ci_prob[1], ", ", ci_prob[2], "]\n")
cat("Model 2 Filtered 95% CI: [", ci_prob_filtered[1], ", ", ci_prob_filtered[2], "]\n")
cat("Model 2 Filtered 2 95% CI: [", ci_prob_filtered_2[1], ", ", ci_prob_filtered_2[2], "]\n")

# None of the comparisons between models show statistically significant differences
# filtering out influential participants did not substantially alter the estimated probabilities, , however it comes closer to a fair coin (p=0.5)


# Are there still influential participants?
inf_mod_filtered <- influence(model_2_filtered_2, group = "person")
plot(inf_mod_filtered, which = "cook")

# Check model assumptions again for filtered models
check_model(model_2_filtered)
check_model(model_2_filtered_2)

# The original model overfit due to influential participants dominating variability, effectively masking residual issues.
# The filtered model became more precise, but revealed residual patterns that need further exploration.

# Plot residuals against fitted values
plot(resid(model_2_filtered_2) ~ fitted(model_2_filtered_2), 
     xlab = "Fitted Values", 
     ylab = "Residuals", 
     main = "Residuals vs Fitted Values for Model 2 Filtered 2")
abline(h = 0, col = "red", lty = 2)
# Residuals are roughly centered around 0, with no strong systematic bias in the mean.
# Vertical clustering suggests grouped dataâ€”due to participants contributing multiple observations.
# Fanning Residuals could indicate overdispersion (variance higher than expected under a binomial model). Lets test for overdispersion
overdispersion_test <- sum(residuals(model_2_filtered_2, type = "pearson")^2) / df.residual(model_2_filtered_2)
cat("Overdispersion Statistic:", overdispersion_test, "\n")
# mild overdispersion, binomial model is not severely mis-specified
# QQ plot for residuals
qqnorm(resid(model_2_filtered_2))
qqline(resid(model_2_filtered_2), col = "red")
# looks like a reasonably good fit
```

```{r, message=FALSE, warning=FALSE}
#| echo: true
# extract fixed and random effects
fixed_effects_participants <- tidy(model_2, effects = "fixed")
random_effects_participants <- tidy(model_2, effects = "ran_pars")

fixed_effects_participants
random_effects_participants

```

##### 2.3 Random Intercepts for Participants and Coins Nested Within Participants

```{r, message=FALSE, warning=FALSE}
#| echo: true
# Mixed-effects logistic regression model with nested random intercepts
model_3 <- glmer(cbind(total_same_side, total_flips - total_same_side) ~ 1 + 
                      (1 | person/coin),
                    family = binomial, data = df_agg)
# Print Summary
summary(model_3)

# Model diagnostics
check_model(model_3)
```
```{r}
# Which participants, if removed entirely, change the model fit the most?
inf_mod3 <- influence(model_3, group = "person")
summary(inf_mod3)
plot(inf_mod3, which = "cook")

# Extract Cook's distances for each participant
cook_vals <- cooks.distance(inf_mod3)
cook_df <- data.frame(
  person = levels(df_agg$person),
  cooks  = as.numeric(cook_vals)
)

# Sort them descending
cook_df_sorted <- cook_df[order(-cook_df$cooks), ]
head(cook_df_sorted, 10)

# Refit model without the most influential participant and the two most influential participants
df_filtered <- df_agg %>%
  filter(!(person %in% c("TianqiPeng")))

df_filtered_2 <- df_agg %>%
  filter(!(person %in% c("TianqiPeng", "JanYang")))

model_3_filtered <- glmer(cbind(total_same_side, total_flips - total_same_side) ~ 1 + 
                      (1 | person/coin),
                    family = binomial, data = df_filtered)

model_3_filtered_2 <- glmer(cbind(total_same_side, total_flips - total_same_side) ~ 1 + 
                      (1 | person/coin),
                    family = binomial, data = df_filtered_2)

summary(model_3_filtered)
summary(model_3_filtered_2)

# Again, filtering out the two influential participants (TianqiPeng and JanYang) led to a better-fitting model, without changing the substantive conclusions.
# Coins exhibit low variance in all models, reinforcing that variability is mostly participant-driven.

## Investigating probability of same side outcome 
# Extract fixed effect estimates (log-odds) for all models
beta_0 <- fixef(model_3)["(Intercept)"]
beta_0_filtered <- fixef(model_3_filtered)["(Intercept)"]
beta_0_filtered_2 <- fixef(model_3_filtered_2)["(Intercept)"]

# Calculate standard errors
se <- sqrt(vcov(model_2)["(Intercept)", "(Intercept)"])
se_filtered <- sqrt(vcov(model_3_filtered)["(Intercept)", "(Intercept)"])
se_filtered_2 <- sqrt(vcov(model_3_filtered_2)["(Intercept)", "(Intercept)"])

# Compute probabilities
prob <- exp(beta_0) / (1 + exp(beta_0))
prob_filtered <- exp(beta_0_filtered) / (1 + exp(beta_0_filtered))
prob_filtered_2 <- exp(beta_0_filtered_2) / (1 + exp(beta_0_filtered_2))

# Display probabilities
cat("Model 3 Probability: ", prob, "\n")
cat("Model 3 Filtered Probability: ", prob_filtered, "\n")
cat("Model 3 Filtered 2 Probability: ", prob_filtered_2, "\n")

# Test differences in log-odds (z-tests)
# Model 2 vs Model 2 Filtered
z_1 <- (beta_0 - beta_0_filtered) / sqrt(se^2 + se_filtered^2)
p_1 <- 2 * (1 - pnorm(abs(z_1)))

# Model 2 vs Model 2 Filtered 2
z_2 <- (beta_0 - beta_0_filtered_2) / sqrt(se^2 + se_filtered_2^2)
p_2 <- 2 * (1 - pnorm(abs(z_2)))

# Model 2 Filtered vs Model 2 Filtered 2
z_3 <- (beta_0_filtered - beta_0_filtered_2) / sqrt(se_filtered^2 + se_filtered_2^2)
p_3 <- 2 * (1 - pnorm(abs(z_3)))

# Display z-scores and p-values for significance testing
cat("Model 2 vs Model 2 Filtered: z =", z_1, ", p =", p_1, "\n")
cat("Model 2 vs Model 2 Filtered 2: z =", z_2, ", p =", p_2, "\n")
cat("Model 2 Filtered vs Model 2 Filtered 2: z =", z_3, ", p =", p_3, "\n")

# Confidence intervals for probabilities
ci <- beta_0 + c(-1.96, 1.96) * se
ci_filtered <- beta_0_filtered + c(-1.96, 1.96) * se_filtered
ci_filtered_2 <- beta_0_filtered_2 + c(-1.96, 1.96) * se_filtered_2

# Transform to probability scale
ci_prob <- exp(ci) / (1 + exp(ci))
ci_prob_filtered <- exp(ci_filtered) / (1 + exp(ci_filtered))
ci_prob_filtered_2 <- exp(ci_filtered_2) / (1 + exp(ci_filtered_2))

# Display confidence intervals
cat("Model 2 95% CI: [", ci_prob[1], ", ", ci_prob[2], "]\n")
cat("Model 2 Filtered 95% CI: [", ci_prob_filtered[1], ", ", ci_prob_filtered[2], "]\n")
cat("Model 2 Filtered 2 95% CI: [", ci_prob_filtered_2[1], ", ", ci_prob_filtered_2[2], "]\n")

# None of the comparisons between models show statistically significant differences
# filtering out influential participants did not substantially alter the estimated probabilities, however it comes closer to a fair coin (p=0.5)

# Are there still influential participants?
inf_mod_filtered <- influence(model_3_filtered_2, group = "person")
plot(inf_mod_filtered, which = "cook")

# Check model assumptions again for filtered models
check_model(model_3_filtered)
check_model(model_3_filtered_2)

performance::check_overdispersion(model_3_filtered_2)
# variance is only slightly larger than expected under a binomial distribution
# binomial model assumptions are not perfectly met

# Plot residuals against fitted values
plot(resid(model_3_filtered_2) ~ fitted(model_3_filtered_2), 
     xlab = "Fitted Values", 
     ylab = "Residuals", 
     main = "Residuals vs Fitted Values for Model 3 Filtered 2")
abline(h = 0, col = "red", lty = 2)
# A weak upward trend is noticeable as fitted values increase.
# this aligns with the overdispersion ratio = 1.035 and p = 0.032 discussed earlier.

# QQ plot for residuals
qqnorm(resid(model_3_filtered_2))
qqline(resid(model_3_filtered_2), col = "red")
# slight deviations in tails, consistent with overdispersion or non-normality in residuals. The deviations are not extreme, suggesting the model is reasonably well-specified, but minor improvements could further stabilize the fit.
```

```{r, message=FALSE, warning=FALSE}
#| echo: true
# extract fixed and random effects
fixed_effects_nested <- tidy(model_3, effects = "fixed")
random_effects_nested <- tidy(model_3, effects = "ran_pars")

fixed_effects_nested
random_effects_nested

```
#### Comparison of Models 1,2 and 3

```{r, message=FALSE, warning=FALSE}
#| echo: true
# Summaries of AIC, BIC, logLik, deviance
model_comp <- rbind(
  data.frame(
    Model       = "Model 1: Simple Logistic",
    AIC         = AIC(model_1),
    BIC         = BIC(model_1),
    logLik      = as.numeric(logLik(model_1)),
    Deviance    = deviance(model_1)
  ),
  data.frame(
    Model       = "Model 2: Participants Only",
    AIC         = AIC(model_2),
    BIC         = BIC(model_2),
    logLik      = as.numeric(logLik(model_2)),
    Deviance    = deviance(model_2)
  ),
  data.frame(
    Model       = "Model 3: Participants + Nested Coins",
    AIC         = AIC(model_3),
    BIC         = BIC(model_3),
    logLik      = as.numeric(logLik(model_3)),
    Deviance    = deviance(model_3)
  )
)

model_comp

```

We use likelihood ratio tests (LRTs) to formally compare the models:

```{r, message=FALSE, warning=FALSE}
#| echo: true

anova(model_1, model_2, model_3, test = "Chisq")

```


```{r, message=FALSE, warning=FALSE}
#| echo: true
# Comparing fixed effects

# For model_1 (GLM):
fixef_1 <- tidy(model_1) %>%
  select(term, estimate, std.error, statistic, p.value) %>%
  mutate(Model = "Model 1")

# For model_2 (mixed-effects):
fixef_2 <- tidy(model_2, effects = "fixed") %>%
  select(term, estimate, std.error, statistic, p.value) %>%
  mutate(Model = "Model 2")

# For model_3 (mixed-effects):
fixef_3 <- tidy(model_3, effects = "fixed") %>%
  select(term, estimate, std.error, statistic, p.value) %>%
  mutate(Model = "Model 3")


fixed_all <- rbind(fixef_1, fixef_2, fixef_3)
fixed_all


```

```{r, message=FALSE, warning=FALSE}
#| echo: true

#Comparing random effects
ranef_2 <- tidy(model_2, effects = "ran_pars") %>% mutate(Model = "Model 2")
ranef_3 <- tidy(model_3, effects = "ran_pars") %>% mutate(Model = "Model 3")

rbind(ranef_2, ranef_3)

```
##### 2.4 Influence of Recent Flips
```{r, message=FALSE, warning=FALSE}
#| echo: true
lag_models <- list(
  lag1 = glm(toss_end ~ lag_1, family=binomial, data=df_agg_time),
  lag2 = glm(toss_end ~ lag_1 + lag_2, family=binomial, data=df_agg_time),
  lag3 = glm(toss_end ~ lag_1 + lag_2 + lag_3, family=binomial, data=df_agg_time)
)

# Compare models using AIC and Likelihood Ratio Tests
AIC(lag_models$lag1, lag_models$lag2, lag_models$lag3)
anova(lag_models$lag1, lag_models$lag2, test="Chisq")
anova(lag_models$lag2, lag_models$lag3, test="Chisq")

```

######  Markov Chain Analysis
```{r, message=FALSE, warning=FALSE}
#| echo: true
transition_table <- table(df_agg_time$lag_1, df_agg_time$toss_end)
transition_table
chisq.test(transition_table)

transition_plot_df <- df_agg_time %>%
  filter(!is.na(lag_1)) %>%
  mutate(
    prev_flip = factor(lag_1, levels = c("h", "t")),
    curr_flip = factor(toss_end, levels = c("h", "t"))
  ) %>%
  group_by(prev_flip, curr_flip) %>%
  tally() %>%
  group_by(prev_flip) %>%
  mutate(prob = n / sum(n)) %>%
  ungroup()

ggplot(transition_plot_df, aes(x = prev_flip, y = prob, fill = curr_flip)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Transition Probabilities (Markov Chain Analysis)",
    x = "Previous Flip Outcome",
    y = "Probability"
  ) +
  scale_fill_viridis_d(name = "Current Flip") +
  theme_minimal()

```

##### 2.5 Learning Effects 

```{r, message=FALSE, warning=FALSE}
#| echo: true
df_agg_time <- df_agg_time %>%
  group_by(person, sequence_id) %>%
  mutate(flip_index = row_number()) %>%
  ungroup()

model_learning <- glm((toss_start == toss_end) ~ flip_index,
                      family=binomial,
                      data=df_agg_time)
summary(model_learning)

null_learning <- glm((toss_start == toss_end) ~ 1,
                     family=binomial,
                     data=df_agg_time)
anova(null_learning, model_learning, test="Chisq")
# Profile Likelihood Confidence Intervals
confint(model_learning)

#check_model(model_learning)

```

###### 2.6 Binomial Approximation (WLS)

```{r, message=FALSE, warning=FALSE}
#| echo: true
binomial_data <- df_agg_time %>%
  group_by(person, coin) %>%
  summarise(
    total_flips = n(),
    total_success = sum(toss_start == toss_end),
    proportion_success = total_success / total_flips,
    weight = 4 * total_flips
  ) %>%
  ungroup()

wls_model <- lm(proportion_success ~ 1, weights = weight, data = binomial_data)

print(summary(wls_model))

# Residual diagnostics for WLS
binomial_data <- binomial_data %>%
  mutate(
    fitted_values = predict(wls_model),
    residuals = residuals(wls_model)
  )

ggplot(binomial_data, aes(x = fitted_values, y = residuals)) +
  geom_point(alpha = 0.5) +
  labs(title = "Residual Plot for WLS Model", x = "Fitted Values", y = "Residuals") +
  theme_minimal()
check_model(wls_model)

# Remove the two identified outliers (TianqiPeng and JanYang)
df_filtered_2 <- df_agg_time %>%
  filter(!(person %in% c("TianqiPeng", "JanYang")))

# Prepare Data for WLS Model
binomial_data_filtered <- df_filtered_2 %>%
  group_by(person, coin) %>%
  summarise(
    total_flips = n(),
    total_success = sum(toss_start == toss_end),
    proportion_success = total_success / total_flips,
    weight = 4 * total_flips,
    .groups = "drop"
  )

# Fit Weighted Least Squares (WLS) Model
wls_model_filtered <- lm(proportion_success ~ 1, weights = weight, data = binomial_data_filtered)

# Summarize the WLS Model
print(summary(wls_model_filtered))

# Residual Diagnostics
# Add fitted values and residuals for diagnostics
binomial_data_filtered <- binomial_data_filtered %>%
  mutate(
    fitted_values = predict(wls_model_filtered),
    residuals = residuals(wls_model_filtered)
  )

# Plot residuals
ggplot(binomial_data_filtered, aes(x = fitted_values, y = residuals)) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Residual Plot for Filtered WLS Model",
    x = "Fitted Values",
    y = "Residuals"
  ) +
  theme_minimal()

# Perform Model Diagnostics
cat("\nCheck Model Diagnostics for Filtered WLS Model:\n")
check_model(wls_model_filtered)

# Fit WLS Model on Full Dataset
# Prepare the data for the full dataset
binomial_data_full <- df_agg_time %>%
  group_by(person, coin) %>%
  summarise(
    total_flips = n(),
    total_success = sum(toss_start == toss_end),
    proportion_success = total_success / total_flips,
    weight = 4 * total_flips,
    .groups = "drop"
  )

# Fit WLS model on the full dataset
wls_model_full <- lm(proportion_success ~ 1, weights = weight, data = binomial_data_full)

# Get overall probability estimate from the full dataset
overall_prob_full <- coef(wls_model_full)["(Intercept)"]

# Fit WLS Model on Filtered Dataset
# Prepare the data for the filtered dataset
binomial_data_filtered <- df_agg_time %>%
  filter(!(person %in% c("TianqiPeng", "JanYang"))) %>%
  group_by(person, coin) %>%
  summarise(
    total_flips = n(),
    total_success = sum(toss_start == toss_end),
    proportion_success = total_success / total_flips,
    weight = 4 * total_flips,
    .groups = "drop"
  )

# Fit WLS model on the filtered dataset
wls_model_filtered <- lm(proportion_success ~ 1, weights = weight, data = binomial_data_filtered)

# Get overall probability estimate from the filtered dataset
overall_prob_filtered <- coef(wls_model_filtered)["(Intercept)"]

# Compare Overall Probabilities
cat("Overall Probability (Full Dataset):", overall_prob_full, "\n")
cat("Overall Probability (Filtered Dataset):", overall_prob_filtered, "\n")

# Difference in probabilities
prob_diff <- overall_prob_filtered - overall_prob_full
cat("Change in Overall Probability After Removing Outliers:", prob_diff, "\n")

# Extract Estimates and Standard Errors
# For the full dataset
full_estimate <- coef(wls_model_full)["(Intercept)"]
full_se <- summary(wls_model_full)$coefficients["(Intercept)", "Std. Error"]

# For the filtered dataset
filtered_estimate <- coef(wls_model_filtered)["(Intercept)"]
filtered_se <- summary(wls_model_filtered)$coefficients["(Intercept)", "Std. Error"]

# Perform Z-Test
# Calculate the difference in estimates
estimate_diff <- filtered_estimate - full_estimate

# Calculate the pooled standard error
pooled_se <- sqrt(full_se^2 + filtered_se^2)

# Compute the z-statistic
z_value <- estimate_diff / pooled_se

# Calculate the p-value (two-tailed test)
p_value <- 2 * pnorm(-abs(z_value))

# Output Results
cat("Estimate (Full Dataset):", full_estimate, "\n")
cat("Estimate (Filtered Dataset):", filtered_estimate, "\n")
cat("Difference in Estimates:", estimate_diff, "\n")
cat("Pooled Standard Error:", pooled_se, "\n")
cat("Z-Value:", z_value, "\n")
cat("P-Value:", p_value, "\n")

# Interpretation
if (p_value < 0.05) {
  cat("The change in overall probability is statistically significant (p < 0.05).\n")
} else {
  cat("The change in overall probability is NOT statistically significant (p >= 0.05).\n")
}

```